#from generalized linear models library
logreg <- glm(formula=as.factor(trainDF$V3) ~., family = "binomial", data=trainDF, maxit=100) #binomial to have logreg binary classification, maxit = maximum iterations for convergence
summary(logreg)
probslr <- predict(logreg,newdata=testDF, type = "response") #get probabilities of the predictions
probslr[1:10] #see first 10 prob predictions
predClass <- ifelse(probslr > 0.5, 2, 1) #converting soft predictions (probs) to hard ones (labels), if greater than 0.5 then cat 1, otherwise 2
head(predClass)
confusionMatrix3 <- table(testDF$V3, predClass) #create confusion matrix
confusionMatrix3
accTest <- sum(diag(confusionMatrix3)) / sum(confusionMatrix3) #get accuracy
print(paste('Accuracy on Test data', accTest))
#SUPPORT VECTOR MACHINES or SVM
library(e1071)
svmT1 = svm(as.factor(trainDF$V3) ~ ., data = trainDF, kernel = "linear", scale = FALSE) #SVM using linear kernel - supposing data is linearly separable
print(svmT1)
plot(svmT1, trainDF)
svmT2 = svm(as.factor(trainDF$V3) ~ ., data = trainDF, kernel = "radial", scale = FALSE) #SVM using radial / circular kernel - when data is not linearly separable
print(svmT2)
plot(svmT2, trainDF)
svmT3 = svm(as.factor(trainDF$V3) ~ ., data = trainDF, kernel = "polynomial", scale = FALSE) #SVM using polynomial kernel - when data is not linearly separable
print(svmT3)
plot(svmT3, trainDF)
svmT4 = svm(as.factor(trainDF$V3) ~ ., data = trainDF, kernel = "sigmoid", scale = FALSE) #SVM using sigmoid kernel - when data is not linearly separable
print(svmT4)
plot(svmT4, trainDF)
svmpred <- predict(svmT1, testDF, type="class") #to make it return class values
confusionMatrix3 <- table(testDF$V3, svmpred) #confusion matrix
confusionMatrix3
accTest <- sum(diag(confusionMatrix3)) / sum(confusionMatrix3) #get accuracy
print(paste('Accuracy on Test data', accTest))
# What is the best SVM model for our data? Let's compare all the kernels on SVM for our data.
svmpred1 <- predict(svmT1, testDF, type="class") #prediction - class values for each model
svmpred2 <- predict(svmT2, testDF, type="class")
svmpred3 <- predict(svmT3, testDF, type="class")
svmpred4 <- predict(svmT4, testDF, type="class")
cf1 <- table(testDF$V3, svmpred1) #confusion matrix for each model
cf2 <- table(testDF$V3, svmpred2)
cf3 <- table(testDF$V3, svmpred3)
cf4 <- table(testDF$V3, svmpred4)
accTest1 <- sum(diag(cf1)) / sum(cf1) #getting accuracy for each model
accTest2 <- sum(diag(cf2)) / sum(cf2)
accTest3 <- sum(diag(cf3)) / sum(cf3)
accTest4 <- sum(diag(cf4)) / sum(cf4)
print(paste('Linear', accTest1)) #plotting the accuracy results
print(paste('Radial', accTest2))
print(paste('Polynomial', accTest3))
print(paste('Sigmoid', accTest4))
# clear everything and load required libraries/codes
rm(list=ls())
#loading and plotting data
load("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/Lab/practice_3/Data/kdata.rdata") #expected 3 cols, 2 features and label for this demo
#data plotting - visualization
plot(x[,1],x[,2], col=scales::alpha(x[,3],0.3), pch=20)
# data preparation
set.seed(42) # seed is necessary to make it reproducible
#split the data in proportion 70/30 for training and test purposes.
training_percentage = 70
sample_size <- floor((training_percentage/100) * nrow(x))
train_ind <- sample(seq_len(nrow(x)), size = sample_size)
train_ind
train_set <- x[train_ind, ]
test <- x[-train_ind, ]
train <- train_set[,1:2]
train_classes <- train_set[,3]
trainDF <- as.data.frame(train_set)
testDF <- as.data.frame(test)
library(e1071)
nbmodel = naiveBayes(train, train_classes, laplace = 0)
classes_hat = predict(nbmodel, test,
type = c("class", "raw"), threshold = 0.001, eps = 0)
cat(classes_hat)
library(PlaneGeometry)
ell <- EllipseFromEquation(A = 8, B = 16, C = 8, D = 50, E = -200, F = 200)
box <- ell$boundingbox()
plot(NULL, asp = 1, xlim = box$x, ylim = box$y, xlab = NA, ylab = NA)
draw(ell, col = "yellow", border = "blue", lwd = 2)
install.packages("PlaneGeometry")
library(PlaneGeometry)
ell <- EllipseFromEquation(A = 8, B = 16, C = 8, D = 50, E = -200, F = 200)
box <- ell$boundingbox()
plot(NULL, asp = 1, xlim = box$x, ylim = box$y, xlab = NA, ylab = NA)
draw(ell, col = "yellow", border = "blue", lwd = 2)
library(PlaneGeometry)
ell <- EllipseFromEquation(A = 8, B = 16, C = 8, D = 50, E = -200, F = 200)
library(PlaneGeometry)
ell <- EllipseFromEquation(A = 8, B = 16, C = 50, D = -200, E = 208, F = 1)
box <- ell$boundingbox()
plot(NULL, asp = 1, xlim = box$x, ylim = box$y, xlab = NA, ylab = NA)
draw(ell, col = "yellow", border = "blue", lwd = 2)
library(rgl)
#this is the plane
planes3d(8, -16, 50, 199, col = 'red')
library(rgl)
plot3d(1, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red')
library(rgl)
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red')
ell <- EllipseFromEquation(A = 8, B = 16, C = 50, D = -200, E = 208, F = 1)
box <- ell$boundingbox()
plot(NULL, asp = 1, xlim = box$x, ylim = box$y, xlab = NA, ylab = NA)
draw(ell, col = "yellow", border = "blue", lwd = 2)
library(rgl)
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red')
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlim = c(-1, 1), ylim = c(-1, 1), zlim = c(-3, 3), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red', alpha = 0.6)
library(rgl)
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red', alpha = 0.6)
library(rgl)
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlim = c(-100, 100), ylim = c(-100, 100), zlim = c(-100, 100), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red', alpha = 0.6)
library(PlaneGeometry)
#this is the ellipse
ell <- EllipseFromEquation(A = 8, B = 16, C = 50, D = -200, E = 208, F = 1)
box <- ell$boundingbox()
plot(NULL, asp = 1, xlim = box$x, ylim = box$y, xlab = NA, ylab = NA)
draw(ell, col = "yellow", border = "blue", lwd = 2)
library(rgl)
dat <- replicate(2, 1:3)
plot3d(dat, type = 'n', xlim = c(-100, 100), ylim = c(-100, 100), zlim = c(-100, 100), xlab = '', ylab = '', zlab = '')
#this is the plane
planes3d(8, -16, 50, 199, col = 'red', alpha = 0.6)
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
num_features = ndim(x)[1]
num_features = size(x)[1]
num_features = dim(x)
num_samples, num_features = dim(x)
num_samples = dim(x)[0]
num_features = dim(x)[1]
num_samples = dim(x)[0]
num_features = dim(x)[2]
num_samples = dim(x)[1]
global_mean = mean(x)
my_mean = sum(x)/num_samples
my_mean = sum(x)/(2*num_samples)
load("./dataset.RData")
x=dataset
num_features = dim(x)[2]
num_samples = dim(x)[1]
x=dataset$points
num_features = dim(x)[2]
num_samples = dim(x)[1]
#assumes dataset with labels included
num_features = dim(x)[2]-1
num_samples = dim(x)[1]
# can we use the mean / std function?
global_mean = mean(x)
my_mean = sum(x)/(2*num_samples)
feature_mean = list()
feature_std = list()
labels = x[,num_features+1]
# can we use the mean / std function?
global_mean = mean(x)
my_mean = sum(x)/(2*num_samples)
# can we use the mean / std function?
global_mean = mean(x[,1:2])
my_mean = sum(x)/(2*num_samples)
feature_mean = list()
feature_std = list()
features = x[,1:num_features]
labels = x[,num_features+1]
# can we use the mean / std function?
global_mean = mean(features)
my_mean = sum(features)/(num_features*num_samples)
feature_mean = list()
feature_std = list()
View(features)
View(x)
num_classes = unique(labels)
num_classes = length(unique(labels))
num_classes
for(i in seq(along = 1:num_classes)){
print(i)
}
x[,x[,3] == 1]
x[,3] == 1
x[x[,3] == 1]
data_for_class = x[x[,3] == i]
x[x[,num_features+1] == i]
x
length(x[,3] == 1)
unique(labels)
x[x[,num_features+1] == 1,]
2**2
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
print(fischer_score)
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
print(dim(data_for_class))
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
?Sd
?sd
?mean
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
rm(list=ls())
set.seed(1337)
packages_used = c("rstudioapi")
for(package in packages_used){
if(package %in% rownames(installed.packages()) == FALSE) {
install.packages(package)
}
}
setwd_current_path = function(){
library(rstudioapi)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path)) #get this current folder
#setwd('..') #go 1 up for scalability
print(getwd())
}
setwd_current_path()
#source("./Exercise 1. Distance function/oen_minkowski.R")
load("./3Dgauss.RData")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
install.packages("Rdimtools")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
fscore_thirdparty
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
View(generated_data)
plot(fscore_thirdparty$Y, pch=19, col=factor(labels), main="Fisher Score")
rm(list=ls())
data(iris)
force(iris)
View(iris)
x = as.matrix(iris)
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
print(fscore_thirdparty$featidx)
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
data(iris)
rm(list=ls())
data(iris)
force(iris)
View(iris)
iris[0]
iris[1]
x=as.matrix(iris[,1:4])
labels = as.factor(iris[,5])
num_features = dim(x)[2]-1
num_samples = dim(x)[1]
features = x[,1:num_features]
x=as.matrix(iris[,1:5])
4
x=as.matrix(iris[,1:4])
labels = as.factor(iris[,5])
#assumes dataset with labels included
num_features = dim(x)[2]
num_samples = dim(x)[1]
features = x[,1:num_features]
num_classes = length(unique(labels))
# can we use the mean / std function?
mu = mean(features)
my_mean = sum(features)/(num_features*num_samples)
#fischer score list
fischer_score = list()
for(y in seq(along = 1:num_features)){
im_result = 0
sum_top = 0
sum_bottom = 0
for(z in seq(along = 1:num_classes)){
data_for_class = x[x[,num_features+1] == z,]
mu_j = mean(data_for_class[,y], na.rm = TRUE)
std_j = sd(data_for_class[,y], na.rm = TRUE)
p_j = nrow(data_for_class)/nrow(x)
sum_top = sum_top + p_j*(mu_j - mu)**2
sum_bottom = sum_bottom + p_j*(std_j**2)
}
im_result = sum_top/sum_bottom
fischer_score[[y]] = im_result
}
for(y in seq(along = 1:num_features)){
im_result = 0
sum_top = 0
sum_bottom = 0
for(z in seq(along = 1:num_classes)){
data_for_class = x[labels == z,]
mu_j = mean(data_for_class[,y], na.rm = TRUE)
std_j = sd(data_for_class[,y], na.rm = TRUE)
p_j = nrow(data_for_class)/nrow(x)
sum_top = sum_top + p_j*(mu_j - mu)**2
sum_bottom = sum_bottom + p_j*(std_j**2)
}
im_result = sum_top/sum_bottom
fischer_score[[y]] = im_result
}
print(fischer_score)
rm(list=ls())
set.seed(1337)
data(iris)
x=as.matrix(iris[,1:4])
labels = as.factor(iris[,5])
#assumes dataset with labels included
num_features = dim(x)[2]
num_samples = dim(x)[1]
features = x[,1:num_features]
num_classes = length(unique(labels))
# can we use the mean / std function?
mu = mean(features)
my_mean = sum(features)/(num_features*num_samples)
#fischer score list
fischer_score = list()
labels == 1
factor(labels) == 1
for(y in seq(along = 1:num_features)){
for(y in seq(along = 1:num_features)){
im_result = 0
sum_top = 0
sum_bottom = 0
for(z in labels){
data_for_class = x[x[,labels == z,]
mu_j = mean(data_for_class[,y], na.rm = TRUE)
std_j = sd(data_for_class[,y], na.rm = TRUE)
p_j = nrow(data_for_class)/nrow(x)
sum_top = sum_top + p_j*(mu_j - mu)**2
sum_bottom = sum_bottom + p_j*(std_j**2)
}
im_result = sum_top/sum_bottom
fischer_score[[y]] = im_result
}
print(fischer_score)
fscore_thirdparty = do.fscore(features, labels)
print("Most important features according to 3rd party: ")
print(fscore_thirdparty$featidx)
plot(fscore_thirdparty$Y, pch=19, col=factor(labels), main="Fisher Score")
rm(list=ls())
set.seed(1337)
data(iris)
x=as.matrix(iris[,1:4])
labels = as.factor(iris[,5])
#assumes dataset with labels included
num_features = dim(x)[2]
num_samples = dim(x)[1]
features = x[,1:num_features]
num_classes = length(unique(labels))
# can we use the mean / std function?
mu = mean(features)
my_mean = sum(features)/(num_features*num_samples)
#fischer score list
fischer_score = list()
for(y in seq(along = 1:num_features)){
im_result = 0
sum_top = 0
sum_bottom = 0
for(z in labels){
data_for_class = x[x[,labels] == z,]
mu_j = mean(data_for_class[,y], na.rm = TRUE)
std_j = sd(data_for_class[,y], na.rm = TRUE)
p_j = nrow(data_for_class)/nrow(x)
sum_top = sum_top + p_j*(mu_j - mu)**2
sum_bottom = sum_bottom + p_j*(std_j**2)
}
im_result = sum_top/sum_bottom
fischer_score[[y]] = im_result
}
x[x[,labels] == "setosa",]
x[x[,labels] == "setosa"]
x[x[,labels == "setosa",]
x
data_for_class = as.matrix(iris[iris[,5] == z,1:4])
source("~/.active-rstudio-document")
rm(list=ls())
set.seed(1337)
data(iris)
x=as.matrix(iris[,1:4])
labels = as.factor(iris[,5])
#assumes dataset with labels included
num_features = dim(x)[2]
num_samples = dim(x)[1]
features = x[,1:num_features]
#labels = x[,num_features+1]
num_classes = length(unique(labels))
# can we use the mean / std function?
mu = mean(features)
my_mean = sum(features)/(num_features*num_samples)
#fischer score list
fischer_score = list()
for(y in seq(along = 1:num_features)){
im_result = 0
sum_top = 0
sum_bottom = 0
for(z in labels){
data_for_class = as.matrix(iris[iris[,5] == z,1:4])
mu_j = mean(data_for_class[,y], na.rm = TRUE)
std_j = sd(data_for_class[,y], na.rm = TRUE)
p_j = nrow(data_for_class)/nrow(x)
sum_top = sum_top + p_j*(mu_j - mu)**2
sum_bottom = sum_bottom + p_j*(std_j**2)
}
im_result = sum_top/sum_bottom
fischer_score[[y]] = im_result
}
print(fischer_score)
fscore_thirdparty = do.fscore(features, labels)
print("Most important features according to 3rd party: ")
print(fscore_thirdparty$featidx)
plot(fscore_thirdparty$Y, pch=19, col=factor(labels), main="Fisher Score")
fscore_thirdparty
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
x11()
plot(x[,1], x[,2], pch=19, col=factor(labels), main="Fisher Score")
mean(x[,4])
x[,4]
mean(data_for_class[,y]
mean(data_for_class[,y])
data_for_class
data_for_class[,y]
data_for_class
iris.columns
View(iris)
x.columns
x
x[,1]
data_for_class
data_for_class[1]
data_for_class[,1]
data_for_class
data_for_class.columns
sorted(data_for_class)
data_for_class$sepal.length
data_for_class$Sepal.Length
data_for_class[, "Sepal.Length"]
View(data_for_class)
View(x)
source("~/.active-rstudio-document")
iris
colnames(iris)
colnames(iris)[1:4]
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
print("Most important features according to 3rd party: ")
print(fscore_thirdparty$featidx)
plot(fscore_thirdparty$Y, pch=19, col=factor(labels), main="Fisher Score")
x11()
plot(x[,1], x[,2], pch=19, col=factor(labels), main="Fisher Score")
data_classes
data_for_class
data_for_class = iris$Species == z
source("~/.active-rstudio-document")
data_for_class = subset(iris, iris$Species == z, select = y)
data_for_class
as.matrix(data_for_class)
data_for_class = as.matrix(subset(iris, iris$Species == z, select = y))
source("~/.active-rstudio-document")
data_for_class = as.matrix(subset(iris, iris$Species == z, select = y))
data_for_class
mean(data_for_class)
mean(iris$Petal.Width)
iris$y
y
data_for_class = as.matrix(subset(iris, iris$Species == z, select = y))
View(data_for_class)
data_for_class = as.matrix(subset(iris, iris$Species == z, select = y))
data_for_class
data_for_class$Petal.Width
data_for_class[,1]
data_for_class[,2]
data_for_class[1,]
data_for_class[2,]
mean(data_for_class)
mean(iris$Petal.Width)
iris$Petal.Width
nrow(data_for_class)
p_j
mu_j
mu
std_j
source("~/.active-rstudio-document")
data_for_class
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
sort(fischer_score)
fischer_score
source("~/.active-rstudio-document")
as.matrix(iris[iris[,5] == z,y])
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("~/.active-rstudio-document")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
data_for_class
nrow(data_for_class)
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
mean(x[,2])
x
mean(x[,3])
data_for_class
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score.R")
source("G:/My Drive/1. EIT Digital master/Estland/Semester 1/Data mining/data-mining-iti8730-hw2/fischer-score-validation.R")
